)
tibble::tibble
ab <- new_abstracts(
doi = example_doi,
title = example_title,
abstract = example_abstract,
class = example_class
)
ab
tibble:::print.tbl_df(ab)
ab
print(ab)
class(ab)
ab <- new_abstracts(
doi = example_doi,
title = example_title,
abstract = example_abstract,
class = example_class
)
class(ab)
ab
class(ab)
tibble:::print.tbl_df
get_abstracts(lacsSample)
get_abstracts(lacsSample)
lacsSample
get_abstracts(lacs)
get_abstracts(lacsSample) |> class()
document()
new_abstracts(
doi = example_doi,
title = example_title,
abstract = example_abstract,
class = example_class
)
new_abstracts(
doi = example_doi,
title = example_title,
abstract = example_abstract,
class = example_class
)
term_count
abstracts <- get_abstracts(lacsSample)
abstracts
v <- get_vocabulary(abstracts)
tolower
usethis::use_package("text2vec")
usethis::use_package("dplyr)
usethis::use_package("dplyr")
usethis::use_package("stopwords")
document()
usethis::use_package("rlang")
document()
new_abstracts(
doi = example_doi,
title = example_title,
abstract = example_abstract,
class = example_class
)
library(lacs)
new_abstracts(
doi = example_doi,
title = example_title,
abstract = example_abstract,
class = example_class
)
example_doi
glmnet:::print.cv.glmnet
tibble:::print.tbl_df
library(devtools)
document()
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
version
install.packages("Matrix")
install.packages("Matrix")
install.packages("stopwords")
install.packages("dplyr")
install.packages("text2vec")
librar(devtools)
library(devtools)
install.packages("devtools")
install.packages("usethis")
devtools::document()
abstracts
abstracts$abstract
abstracts$abstract
class(abstracts$abstract)
library(devtools)
document()
use_package("utf8")
class(abstracts)
is((abstracts))
document()
document()
document()
use_packages("methods")
usethis::use_package("methods")
document()
document()
get_vocabulary(abstracts)
abstracts$abstract
text <- abstracts$abstract
text  <- utf8::utf8_format(text)
#loadpkg("stringi")
prep_fun <- tolower
text
it = text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
stop_words <- stopwords::stopwords()
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
text  <- utf8::utf8_normalize(text)
text  <- utf8::utf8_format(text)
text  <- utf8::utf8_normalize(text)
text  <- utf8::utf8_encode(text)
text  <- utf8::utf8_normalize(text)
text
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
text  <- utf8::utf8_encode(text)
text  <- utf8::utf8_normalize(text)
#loadpkg("stringi")
prep_fun <- tolower
it = text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
stop_words <- stopwords::stopwords()
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v
v <- v |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v
pruned_vocab = text2vec::prune_vocabulary(v, term_count_min = term_count)
text  <- utf8::utf8_format(text)
text  <- utf8::utf8_normalize(text)
#loadpkg("stringi")
prep_fun <- tolower
it = text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
stop_words <- stopwords::stopwords()
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v <- v |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v
____
?stringer::str_remove
?stringer::str_remove
install.packages("stringer")
install.packages("stringr")
?stringr::str_remove
v <- v |>
dplyr::mutate(.data$term = gsub("[[:punct:]]", "", .data$term))
v <- v |>
dplyr::mutate(.data$term = gsub("[[:punct:]]", "", .data$term)) |>
v |>
dplyr::mutate(term = gsub("[[:punct:]]", "", .data$term))
v |>
dplyr::mutate(.data$term = gsub("[[:punct:]]", "", .data$term))
v |>
dplyr::mutate(term = gsub("[[:punct:]]", "", .data$term))
v |>
dplyr::mutate(term = gsub("[[:punct:]]", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v |>
dplyr::mutate(term = gsub("[[:punct:]]", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v <- v |>
#dplyr::mutate(term = gsub("[[:punct:]]", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v
v |>
dplyr::mutate(term = gsub("__", "", .data$term))
v <- v |>
dplyr::mutate(term = gsub("__", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v
pruned_vocab = text2vec::prune_vocabulary(v, term_count_min = term_count)
docu
library(devtools)
document()
usethis::use_pkgdown_github_pages()
abstracts <- get_abstracts(lacsSample)
abstracts
v <- get_vocabulary(abstracts)
v
lacs
abstracts <- get_abstracts(lacs)
v <- get_vocabulary(abstracts)
v
v
lacs
text  <- utf8::utf8_encode(text)
text  <- utf8::utf8_format(text)
text  <- utf8::utf8_normalize(text)
text <- lacs$abstract
text  <- utf8::utf8_encode(text)
text  <- utf8::utf8_format(text)
text  <- utf8::utf8_normalize(text)
#loadpkg("stringi")
prep_fun <- tolower
#loadpkg("stringi")
prep_fun <- tolower
it = text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
it <-  text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
it
stop_words <- stopwords::stopwords()
stop_words
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v
v <- v |>
dplyr::mutate(term = gsub("___", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v
v |>
dplyr::mutate(term = gsub("___", "", .data$term))
v |>
dplyr::mutate(term = gsub("___", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term))
v |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v
pruned_vocab = text2vec::prune_vocabulary(v, term_count_min = term_count)
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
document()
abstracts <- get_abstracts(lacsSample)
v <- get_vocabulary(abstracts)
v
abstracts <- get_abstracts(lacs)
v <- get_vocabulary(abstracts)
v
v
text <- lacs$abstract
text  <- utf8::utf8_encode(text)
text  <- utf8::utf8_format(text)
text  <- utf8::utf8_normalize(text)
#loadpkg("stringi")
prep_fun <- tolower
it <-  text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
stop_words <- stopwords::stopwords()
it <-  text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
stop_words <- stopwords::stopwords()
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v
text
text
text
#loadpkg("stringi")
prep_fun <- tolower
prep_fun
it <-  text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
it
abstracts <- get_abstracts(lacs)
v <- get_vocabulary(abstracts)
v
lacsSample
abstracts <- get_abstracts(lacsSample)
v <- get_vocabulary(abstracts)
v
abstracts <- get_abstracts(lacs)
v <- get_vocabulary(abstracts)
v
v
v
text  <- utf8::utf8_encode(text)
text  <- utf8::utf8_format(text)
text  <- utf8::utf8_normalize(text)
#loadpkg("stringi")
prep_fun <- tolower
it <-  text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
stop_words <- stopwords::stopwords()
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v
pruned_vocab = text2vec::prune_vocabulary(v, term_count_min = term_count)
pruned_vocab = text2vec::prune_vocabulary(v, term_count_min = 2)
pruned_vocab
names(pruned_vocab) <- "vocabulary"
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 2)
pruned_vocab = text2vec::prune_vocabulary(v, term_count_min = 2)
pruned_vocab
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 2)
pruned_vocab = text2vec::prune_vocabulary(v, term_count_min = 2)
pruned_vocab
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v
text  <- utf8::utf8_encode(text)
text  <- utf8::utf8_format(text)
text  <- utf8::utf8_normalize(text)
#loadpkg("stringi")
prep_fun <- tolower
it <-  text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
stop_words <- stopwords::stopwords()
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 2)
v
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v
pruned_vocab = text2vec::prune_vocabulary(v, term_count_min = 2)
pruned_vocab
names(pruned_vocab) <- "vocabulary"
text  <- utf8::utf8_encode(text)
#text  <- utf8::utf8_format(text)
#text  <- utf8::utf8_normalize(text)
#loadpkg("stringi")
prep_fun <- tolower
it <-  text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
stop_words <- stopwords::stopwords()
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v
pruned_vocab = text2vec::prune_vocabulary(v, term_count_min = 2)
names(pruned_vocab) <- "vocabulary"
pruned_vocab
text <- lacs$abstract
text  <- utf8::utf8_encode(text)
text  <- utf8::utf8_format(text)
text  <- utf8::utf8_normalize(text)
text <- lacs$abstract
it <-  text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
stop_words <- stopwords::stopwords()
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
pruned_vocab = text2vec::prune_vocabulary(v, term_count_min = 2)
pruned_vocab
text <- lacs$abstract
text
text <- lacs$abstract
text  <- utf8::utf8_encode(text)
#text  <- utf8::utf8_format(text)
#text  <- utf8::utf8_normalize(text)
#loadpkg("stringi")
prep_fun <- tolower
it <-  text2vec::itoken(text,
preprocessor = prep_fun,
#tok_fun = word_tokenizer,
progressbar = FALSE)
stop_words <- stopwords::stopwords()
v <-  text2vec::create_vocabulary(it,
ngram = c(1L, 5L),
stopwords = stop_words)
v
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 3)
v <- v |>
dplyr::mutate(term = gsub("____", "", .data$term)) |>
dplyr::mutate(term = gsub("…", "", .data$term)) |>
dplyr::filter(!grepl(pattern = "^[0-9]", .data$term )) |>
dplyr::filter(!grepl(pattern = "$[0-9]", .data$term )) |>
# filter all terms with less than three characters
dplyr::filter(nchar(.data$term) > 2)
pruned_vocab = text2vec::prune_vocabulary(v, term_count_min = 2)
pruned_vocab
document()
